{"paragraphs":[{"text":"%md\n\n# TASK 1: PERSONA CREATION\n\n## Introduction\n\nThe marketing department of PyBank is designing a new marketing campaign for a new deposit. In order to improve the response rate of their last campaign, they have reached out to the bank’s customer analytics department looking for help.\n\nOn the their last marketing campaign, they observed that their conversion rate (the percentage of people that bought their product by the total number of people that was reached during the campaign) varied across different segments of their customer base. There are different reasons why this might happen:\n\n1. The marketing campaign was designed for a specific type of customer (persona) and it was not appealing to others.\n2. The new product that was trying to be pushed into the segment was not relevant (for example, a new deposit for lump sums of more than 5,000 euro might be relevant for people in their 30s but not for students or families).\n3. The value proposition of the product was not good enough for specific segments (e.g. the interest rate for small deposits that students can afford is under the market rate).\n4. The market campaign was not using the right channel or message.\n5. The customer data is obsolete, imcomplete or erroneous.\n\nThere are some ways data science can help marketing in this situations:\n\n1. **Customer base segmentation:** we can create clusters of customers with homogenous behaviour / demographics depending on the needs of the marketing department. Since clustering is usually an unsupervised technique, it is really important to work together with the marketing deparment in this case to agree on an evaluation of the results of a clustering algorithm (for example, how to measure the compacticity of the clusters or which type of common behaviour they expect to observe on the customer of the same cluster).\n2. **Persona creation:** this is link with the previous task. A persona is basically a representation of an *average* customer in a customer segment. In some cases personas are given real names, pictures or details to help everyone understand why type of customers we have in a segment. An example of a persona would be: John, mid 30s, married, no kids, high income & higher diploma education. In order to create good personas, it is *really* important that the segments or clusters we have just created on the previous step are compact. This is because we are using just a single persona to describe the whole segment and if the customers within it are very different we won't be able to describe them accurately. We will do steps 1 and 2 in this Task.\n3. **Propensity modelling:** we can create machine learning models to identify who is more likely to convert. We will do this in Task 2. The beneficts of Propensity Modelling are:\n    - **Cost optimisation:** sometimes running marketing campaigns is not cheap (specially if you must have someone on the other side of the line calling customers). We can reduce the cost of running our marketing campaign by targeting specifically the customers that are likely to convert. We can identify them using a machine learning model. The customers are ranked from the most likely to convert to the less likely and then we can use lift charts to decide what the optimal number of customer we must target.\n    - **Customer satisfaction:** one of the most important metrics for any customer-facing company is customer satisfaction. Just take a look to Amazon's motto: \"The most customer-centric company in the world!\". nobody likes to receive ads about products that are not relevant for them so we can increase our company's NPS by excluding these customers from our marketing campaigns.\n4. **Data quality:** we can help marketing by analysing our customers data and removing outliers / erroneous records from our databases.\n\nIn this notebook, we are going to focus on 1 and 2. This is what we are going to do:\n\n1. **Data ingestion:** the first step is to read the data files we are going to use in this workshop from our server. We've done everything for you, you just have to make sure you have an available internet connection and you run the cell. The files will be save in your machine `/tmp` folder. Feel free to drop them once we've finished the workshop. Then, we will read the files (`.csv`) into PySpark data frames.\n2. **Basic statistics:** we will show you how to get statistics of a data frame using Spark and how to visualise the results. Then, you'll have to do it on your own for one of the columns in our data.\n3. **Clustering:** we have created a clustering pipeline for you that will create different segments on the data. You are encourage to try different algorithms / parameters and see how it affects the results of the last step.\n4. **Persona creation:** we will write some code to automatically create personas from the results of our clustering pipeline. You'll have to decide which features you use in other to describe your segments and which ones are not good enough.\n\nLet's get our hand in the data! Happy hacking!","user":"anonymous","dateUpdated":"2018-03-12T19:43:25+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>TASK 1: PERSONA CREATION</h1>\n<h2>Introduction</h2>\n<p>The marketing department of PyBank is designing a new marketing campaign for a new deposit. In order to improve the response rate of their last campaign, they have reached out to the bank’s customer analytics department looking for help.</p>\n<p>On the their last marketing campaign, they observed that their conversion rate (the percentage of people that bought their product by the total number of people that was reached during the campaign) varied across different segments of their customer base. There are different reasons why this might happen:</p>\n<ol>\n  <li>The marketing campaign was designed for a specific type of customer (persona) and it was not appealing to others.</li>\n  <li>The new product that was trying to be pushed into the segment was not relevant (for example, a new deposit for lump sums of more than 5,000 euro might be relevant for people in their 30s but not for students or families).</li>\n  <li>The value proposition of the product was not good enough for specific segments (e.g. the interest rate for small deposits that students can afford is under the market rate).</li>\n  <li>The market campaign was not using the right channel or message.</li>\n  <li>The customer data is obsolete, imcomplete or erroneous.</li>\n</ol>\n<p>There are some ways data science can help marketing in this situations:</p>\n<ol>\n  <li><strong>Customer base segmentation:</strong> we can create clusters of customers with homogenous behaviour / demographics depending on the needs of the marketing department. Since clustering is usually an unsupervised technique, it is really important to work together with the marketing deparment in this case to agree on an evaluation of the results of a clustering algorithm (for example, how to measure the compacticity of the clusters or which type of common behaviour they expect to observe on the customer of the same cluster).</li>\n  <li><strong>Persona creation:</strong> this is link with the previous task. A persona is basically a representation of an <em>average</em> customer in a customer segment. In some cases personas are given real names, pictures or details to help everyone understand why type of customers we have in a segment. An example of a persona would be: John, mid 30s, married, no kids, high income &amp; higher diploma education. In order to create good personas, it is <em>really</em> important that the segments or clusters we have just created on the previous step are compact. This is because we are using just a single persona to describe the whole segment and if the customers within it are very different we won&rsquo;t be able to describe them accurately. We will do steps 1 and 2 in this Task.</li>\n  <li><strong>Propensity modelling:</strong> we can create machine learning models to identify who is more likely to convert. We will do this in Task 2. The beneficts of Propensity Modelling are:\n    <ul>\n      <li><strong>Cost optimisation:</strong> sometimes running marketing campaigns is not cheap (specially if you must have someone on the other side of the line calling customers). We can reduce the cost of running our marketing campaign by targeting specifically the customers that are likely to convert. We can identify them using a machine learning model. The customers are ranked from the most likely to convert to the less likely and then we can use lift charts to decide what the optimal number of customer we must target.</li>\n      <li><strong>Customer satisfaction:</strong> one of the most important metrics for any customer-facing company is customer satisfaction. Just take a look to Amazon&rsquo;s motto: &ldquo;The most customer-centric company in the world!&rdquo;. nobody likes to receive ads about products that are not relevant for them so we can increase our company&rsquo;s NPS by excluding these customers from our marketing campaigns.</li>\n    </ul>\n  </li>\n  <li><strong>Data quality:</strong> we can help marketing by analysing our customers data and removing outliers / erroneous records from our databases.</li>\n</ol>\n<p>In this notebook, we are going to focus on 1 and 2. This is what we are going to do:</p>\n<ol>\n  <li><strong>Data ingestion:</strong> the first step is to read the data files we are going to use in this workshop from our server. We&rsquo;ve done everything for you, you just have to make sure you have an available internet connection and you run the cell. The files will be save in your machine <code>/tmp</code> folder. Feel free to drop them once we&rsquo;ve finished the workshop. Then, we will read the files (<code>.csv</code>) into PySpark data frames.</li>\n  <li><strong>Basic statistics:</strong> we will show you how to get statistics of a data frame using Spark and how to visualise the results. Then, you&rsquo;ll have to do it on your own for one of the columns in our data.</li>\n  <li><strong>Clustering:</strong> we have created a clustering pipeline for you that will create different segments on the data. You are encourage to try different algorithms / parameters and see how it affects the results of the last step.</li>\n  <li><strong>Persona creation:</strong> we will write some code to automatically create personas from the results of our clustering pipeline. You&rsquo;ll have to decide which features you use in other to describe your segments and which ones are not good enough.</li>\n</ol>\n<p>Let&rsquo;s get our hand in the data! Happy hacking!</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1519469667705_-2139381574","id":"20180224-105427_1666154236","dateCreated":"2018-02-24T10:54:27+0000","dateStarted":"2018-03-12T19:43:25+0000","dateFinished":"2018-03-12T19:43:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:101"},{"title":"1.1. Data download","text":"%pyspark\n\nimport urllib.request\n\nurllib.request.urlretrieve(\"https://wsleaderboard.herokuapp.com/data/test.snappy.parquet\", \"/tmp/test.snappy.parquet\")\nurllib.request.urlretrieve(\"https://wsleaderboard.herokuapp.com/data/training.snappy.parquet\", \"/tmp/training.snappy.parquet\")","user":"anonymous","dateUpdated":"2018-03-23T10:14:32+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"('/tmp/training.snappy.parquet', <http.client.HTTPMessage object at 0x106ec7978>)\n"}]},"apps":[],"jobName":"paragraph_1520108544734_1641141097","id":"20180303-202224_1472950704","dateCreated":"2018-03-03T20:22:24+0000","dateStarted":"2018-03-23T10:14:32+0000","dateFinished":"2018-03-23T10:14:32+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:102"},{"title":"1.2. Read the csv as a PySpark data frame","text":"%pyspark\n\ndata = sqlContext.read.parquet(\"/tmp/training.snappy.parquet\")\ntest = sqlContext.read.parquet(\"/tmp/test.snappy.parquet\")\ndata.show(10)","user":"anonymous","dateUpdated":"2018-03-24T11:41:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python","editorHide":false,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n|age|    job|marital|  education|default|housing|loan|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp.var.rate|cons.price.idx|cons.conf.idx|euribor3m|nr.employed|  y|\n+---+-------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\n| 18|student| single|high.school|     no|    yes| yes| cellular|  mar|        tue|     103|       1|  999|       0|nonexistent|        -1.8|        92.843|        -50.0|    1.687|     5099.1| no|\n| 19|student| single|   basic.6y|     no|     no|  no| cellular|  mar|        tue|     136|       1|  999|       0|nonexistent|        -1.8|        92.843|        -50.0|    1.556|     5099.1|yes|\n| 19|student| single|   basic.9y|     no|     no|  no| cellular|  apr|        thu|     165|       3|  999|       0|nonexistent|        -1.8|        93.075|        -47.1|     1.41|     5099.1| no|\n| 19|student| single|   basic.9y|     no|     no|  no| cellular|  apr|        tue|      43|       3|  999|       0|nonexistent|        -1.8|        93.075|        -47.1|    1.453|     5099.1| no|\n| 19|student| single|   basic.9y|     no|    yes|  no| cellular|  mar|        fri|     126|       4|  999|       0|nonexistent|        -1.8|        92.843|        -50.0|     1.64|     5099.1|yes|\n| 19|student| single|   basic.9y|unknown|    yes|  no| cellular|  jul|        mon|      87|       4|  999|       0|nonexistent|         1.4|        93.918|        -42.7|     4.96|     5228.1| no|\n| 19|student| single|    unknown|     no|     no|  no| cellular|  apr|        fri|     108|       5|  999|       0|nonexistent|        -1.8|        93.075|        -47.1|    1.405|     5099.1| no|\n| 19|student| single|    unknown|     no|    yes|  no| cellular|  apr|        fri|     156|       1|  999|       0|nonexistent|        -1.8|        93.075|        -47.1|    1.405|     5099.1|yes|\n| 19|student| single|    unknown|     no|    yes|  no|telephone|  apr|        fri|    1161|       5|  999|       0|nonexistent|        -1.8|        93.075|        -47.1|    1.405|     5099.1| no|\n| 20| admin.| single|high.school|     no|     no|  no| cellular|  jul|        wed|     208|       4|  999|       0|nonexistent|         1.4|        93.918|        -42.7|    4.962|     5228.1| no|\n+---+-------+-------+-----------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+\nonly showing top 10 rows\n\n"}]},"apps":[],"jobName":"paragraph_1520108349049_-1444284571","id":"20180303-201909_670268623","dateCreated":"2018-03-03T20:19:09+0000","dateStarted":"2018-03-24T08:34:27+0000","dateFinished":"2018-03-24T08:35:09+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:103"},{"title":"2. Basic statistics","text":"%md\n\n### Plotting using Zeppelin\n\nFirst of all, they want to know some basic demographics of the bank’s customer base. Specifically, they want to explore their customers **education levels** and **job families**. Let's start by computing the distribution of job families and by using Zeppelin graphics interpreter to plot the results:\n\n#### Computing the education levels using SparkSQL\n\nWe will use SparkSQL capabilities for computing some basic statistics. Please get familiar with them reading the SparkSQL docs [here](https://spark.apache.org/sql/).\nIf you have use SQL before it'll be very easy for run to run queries on a dataframe. You can just use the `.sql('SELECT * FROM ...')` method and write your query! You should register your data frame first using the function `.registerTempTable('table name')`\nAlso, don't forget to get familiar with the native SparkSQL methods such as `select()`, `groupBy()` or `limit()`. Here, we are going to use these methods to get some insights about the data we've been given.\nDon't forget to add '%pyspark' at the top of your cell if PySpark is not your default interpreter!\n\n#### Using Zeppelin's %table interpreter\n\nZeppelin comes with a graphic interpreter that we can use to make quick visualizations of our results. You can go into *table* mode by printing a string with `%table` in the first line (don't forget the new line character is `\\n`), your column headers in the second line splitted by tabs (`\\t`) and your values on the subsequent lines (splitted by tabs as well). Please take a look how we have put together this string on the following two cells by applying a lambda function to the results collected from our data.","user":"anonymous","dateUpdated":"2018-03-23T07:52:41+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Plotting using Zeppelin</h3>\n<p>First of all, they want to know some basic demographics of the bank’s customer base. Specifically, they want to explore their customers <strong>education levels</strong> and <strong>job families</strong>. Let&rsquo;s start by computing the distribution of job families and by using Zeppelin graphics interpreter to plot the results:</p>\n<h4>Computing the education levels using SparkSQL</h4>\n<p>We will use SparkSQL capabilities for computing some basic statistics. Please get familiar with them reading the SparkSQL docs <a href=\"https://spark.apache.org/sql/\">here</a>.<br/>If you have use SQL before it&rsquo;ll be very easy for run to run queries on a dataframe. You can just use the <code>.sql(&#39;SELECT * FROM ...&#39;)</code> method and write your query! You should register your data frame first using the function <code>.registerTempTable(&#39;table name&#39;)</code><br/>Also, don&rsquo;t forget to get familiar with the native SparkSQL methods such as <code>select()</code>, <code>groupBy()</code> or <code>limit()</code>. Here, we are going to use these methods to get some insights about the data we&rsquo;ve been given.<br/>Don&rsquo;t forget to add &lsquo;%pyspark&rsquo; at the top of your cell if PySpark is not your default interpreter!</p>\n<h4>Using Zeppelin&rsquo;s %table interpreter</h4>\n<p>Zeppelin comes with a graphic interpreter that we can use to make quick visualizations of our results. You can go into <em>table</em> mode by printing a string with <code>%table</code> in the first line (don&rsquo;t forget the new line character is <code>\\n</code>), your column headers in the second line splitted by tabs (<code>\\t</code>) and your values on the subsequent lines (splitted by tabs as well). Please take a look how we have put together this string on the following two cells by applying a lambda function to the results collected from our data.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520108274079_-1401836310","id":"20180303-201754_362095081","dateCreated":"2018-03-03T20:17:54+0000","dateStarted":"2018-03-23T07:52:41+0000","dateFinished":"2018-03-23T07:52:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:104"},{"title":"2.1. Computing education level counts","text":"%pyspark\n\neducationCounts = data.groupBy('education').count().collect()","user":"anonymous","dateUpdated":"2018-03-23T10:16:18+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"},"editorHide":false,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1519469268684_1026336188","id":"20180218-224547_550167252","dateCreated":"2018-02-24T10:47:48+0000","dateStarted":"2018-03-23T10:16:18+0000","dateFinished":"2018-03-23T10:16:20+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:105"},{"title":"2.2. Plotting education level counts","text":"%pyspark\n\nprint(\"%table\\nEducation Level\\tCount\\n\"+\"\\n\".join(list(map(lambda x: x['education'] + '\\t' + str(x['count']), educationCounts))))","user":"anonymous","dateUpdated":"2018-03-24T11:41:23+0000","config":{"colWidth":12,"enabled":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false},"helium":{}}},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"Education Level\tCount\nhigh.school\t4768\nunknown\t836\nbasic.6y\t1164\nprofessional.course\t2641\nuniversity.degree\t6050\nilliterate\t9\nbasic.4y\t2092\nbasic.9y\t3034\n"}]},"apps":[],"jobName":"paragraph_1520281796370_1111579971","id":"20180305-202956_1538649883","dateCreated":"2018-03-05T20:29:56+0000","dateStarted":"2018-03-23T10:16:27+0000","dateFinished":"2018-03-23T10:16:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:106"},{"title":"2.3. YOUR TURN: Computing job family counts","text":"%md\n\nCount the number of customers on each job family using PySpark (either using .sql or the native method -> preferred). Collect the results in a variable.","user":"anonymous","dateUpdated":"2018-03-24T11:40:29+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorHide":true,"title":true,"editorMode":"ace/mode/markdown","tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Count the number of customers on each job family using PySpark (either using .sql or the native method -&gt; preferred). Collect the results in a variable.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520164431067_-859335713","id":"20180304-115351_1785466766","dateCreated":"2018-03-04T11:53:51+0000","dateStarted":"2018-03-24T11:40:29+0000","dateFinished":"2018-03-24T11:40:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:107"},{"title":"2.4. YOUR TURN: Plotting job family counts","text":"%md\n\nPlot the results you computed on the previous step using Zeppelin's %table interpreter","user":"anonymous","dateUpdated":"2018-03-24T11:40:49+0000","config":{"colWidth":12,"enabled":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false},"helium":{}}},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Plot the results you computed on the previous step using Zeppelin&rsquo;s %table interpreter</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520438217243_-615401672","id":"20180307-155657_309996636","dateCreated":"2018-03-07T15:56:57+0000","dateStarted":"2018-03-24T11:40:49+0000","dateFinished":"2018-03-24T11:40:49+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:108"},{"title":"2.5. YOUR TURN (additional): Explore other features in the data","text":"%md\n\nExplore other features in the data. You can also try the `.describe(...)` method in the dataframes.\n","user":"anonymous","dateUpdated":"2018-03-24T11:39:02+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Explore other features in the data. You can also try the <code>.describe(...)</code> method in the dataframes.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520438279450_274214406","id":"20180307-155759_1571284738","dateCreated":"2018-03-07T15:57:59+0000","dateStarted":"2018-03-24T11:39:02+0000","dateFinished":"2018-03-24T11:39:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:109"},{"title":"3. Clustering","text":"%md\n\nThe next step of our assignment is to create a set of segments on our database. We will use Spark's ML pipelines for it. You can find more information regarding Spark's ML pipelines [here](https://spark.apache.org/docs/latest/ml-pipeline.html).\n\n\n","user":"anonymous","dateUpdated":"2018-03-23T10:21:24+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>The next step of our assignment is to create a set of segments on our database. We will use Spark&rsquo;s ML pipelines for it. You can find more information regarding Spark&rsquo;s ML pipelines <a href=\"https://spark.apache.org/docs/latest/ml-pipeline.html\">here</a>.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1520447852595_1436021906","id":"20180307-183732_2135972670","dateCreated":"2018-03-07T18:37:32+0000","dateStarted":"2018-03-23T10:21:24+0000","dateFinished":"2018-03-23T10:21:24+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:110"},{"title":"3.1. Clustering pipeline","text":"%pyspark\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, Normalizer\n\nstringEncoder = StringIndexer(inputCol=\"job\", outputCol=\"jobInd\")\nencoder = OneHotEncoder(inputCol=\"jobInd\", outputCol=\"jobVec\")\nassembler = VectorAssembler(inputCols=[\"jobVec\", \"age\"],outputCol=\"assembledFeatures\")\nnormalizer = Normalizer(inputCol=\"assembledFeatures\", outputCol=\"features\")\nkmeans = KMeans().setK(20).setSeed(1)\n\npipeline = Pipeline(stages=[stringEncoder, encoder, assembler, normalizer, kmeans])\n\np = pipeline.fit(data)\ntransformed = p.transform(data).cache()\ntransformed.show()","user":"anonymous","dateUpdated":"2018-03-24T15:44:14+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+------------+-------+-----------------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+------+---------------+--------------------+--------------------+----------+\n|age|         job|marital|        education|default|housing|loan|  contact|month|day_of_week|duration|campaign|pdays|previous|   poutcome|emp.var.rate|cons.price.idx|cons.conf.idx|euribor3m|nr.employed|  y|jobInd|         jobVec|   assembledFeatures|            features|prediction|\n+---+------------+-------+-----------------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+------+---------------+--------------------+--------------------+----------+\n| 18|     student| single|      high.school|     no|    yes| yes| cellular|  mar|        tue|     103|       1|  999|       0|nonexistent|        -1.8|        92.843|        -50.0|    1.687|     5099.1| no|  10.0|(11,[10],[1.0])|(12,[10,11],[1.0,...|(12,[10,11],[0.05...|        10|\n| 19|     student| single|         basic.6y|     no|     no|  no| cellular|  mar|        tue|     136|       1|  999|       0|nonexistent|        -1.8|        92.843|        -50.0|    1.556|     5099.1|yes|  10.0|(11,[10],[1.0])|(12,[10,11],[1.0,...|(12,[10,11],[0.05...|        10|\n| 19|     student| single|         basic.9y|     no|     no|  no| cellular|  apr|        thu|     165|       3|  999|       0|nonexistent|        -1.8|        93.075|        -47.1|     1.41|     5099.1| no|  10.0|(11,[10],[1.0])|(12,[10,11],[1.0,...|(12,[10,11],[0.05...|        10|\n| 19|     student| single|         basic.9y|     no|     no|  no| cellular|  apr|        tue|      43|       3|  999|       0|nonexistent|        -1.8|        93.075|        -47.1|    1.453|     5099.1| no|  10.0|(11,[10],[1.0])|(12,[10,11],[1.0,...|(12,[10,11],[0.05...|        10|\n| 19|     student| single|         basic.9y|     no|    yes|  no| cellular|  mar|        fri|     126|       4|  999|       0|nonexistent|        -1.8|        92.843|        -50.0|     1.64|     5099.1|yes|  10.0|(11,[10],[1.0])|(12,[10,11],[1.0,...|(12,[10,11],[0.05...|        10|\n| 19|     student| single|         basic.9y|unknown|    yes|  no| cellular|  jul|        mon|      87|       4|  999|       0|nonexistent|         1.4|        93.918|        -42.7|     4.96|     5228.1| no|  10.0|(11,[10],[1.0])|(12,[10,11],[1.0,...|(12,[10,11],[0.05...|        10|\n| 19|     student| single|          unknown|     no|     no|  no| cellular|  apr|        fri|     108|       5|  999|       0|nonexistent|        -1.8|        93.075|        -47.1|    1.405|     5099.1| no|  10.0|(11,[10],[1.0])|(12,[10,11],[1.0,...|(12,[10,11],[0.05...|        10|\n| 19|     student| single|          unknown|     no|    yes|  no| cellular|  apr|        fri|     156|       1|  999|       0|nonexistent|        -1.8|        93.075|        -47.1|    1.405|     5099.1|yes|  10.0|(11,[10],[1.0])|(12,[10,11],[1.0,...|(12,[10,11],[0.05...|        10|\n| 19|     student| single|          unknown|     no|    yes|  no|telephone|  apr|        fri|    1161|       5|  999|       0|nonexistent|        -1.8|        93.075|        -47.1|    1.405|     5099.1| no|  10.0|(11,[10],[1.0])|(12,[10,11],[1.0,...|(12,[10,11],[0.05...|        10|\n| 20|      admin.| single|      high.school|     no|     no|  no| cellular|  jul|        wed|     208|       4|  999|       0|nonexistent|         1.4|        93.918|        -42.7|    4.962|     5228.1| no|   0.0| (11,[0],[1.0])|(12,[0,11],[1.0,2...|(12,[0,11],[0.049...|        12|\n| 20| blue-collar|married|         basic.4y|     no|     no| yes| cellular|  jul|        tue|     285|       1|  999|       0|nonexistent|         1.4|        93.918|        -42.7|    4.962|     5228.1| no|   1.0| (11,[1],[1.0])|(12,[1,11],[1.0,2...|(12,[1,11],[0.049...|         1|\n| 20|entrepreneur| single|      high.school|     no|     no|  no|telephone|  may|        fri|     217|       2|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.855|     5191.0| no|   6.0| (11,[6],[1.0])|(12,[6,11],[1.0,2...|(12,[6,11],[0.049...|        13|\n| 20|entrepreneur| single|      high.school|     no|     no|  no|telephone|  may|        thu|     238|       1|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.855|     5191.0| no|   6.0| (11,[6],[1.0])|(12,[6,11],[1.0,2...|(12,[6,11],[0.049...|        13|\n| 20|entrepreneur| single|      high.school|     no|     no|  no|telephone|  may|        thu|     598|       3|  999|       0|nonexistent|         1.1|        93.994|        -36.4|    4.855|     5191.0| no|   6.0| (11,[6],[1.0])|(12,[6,11],[1.0,2...|(12,[6,11],[0.049...|        13|\n| 20|    services| single|      high.school|     no|     no|  no|telephone|  jul|        tue|      97|      15|  999|       0|nonexistent|         1.4|        93.918|        -42.7|    4.961|     5228.1| no|   3.0| (11,[3],[1.0])|(12,[3,11],[1.0,2...|(12,[3,11],[0.049...|        16|\n| 20|     student| single|         basic.9y|     no|    yes|  no| cellular|  mar|        thu|     267|       2|  999|       0|nonexistent|        -1.8|        92.843|        -50.0|    1.538|     5099.1| no|  10.0|(11,[10],[1.0])|(12,[10,11],[1.0,...|(12,[10,11],[0.04...|        10|\n| 20|     student| single|      high.school|     no|    yes|  no| cellular|  jul|        mon|     100|       4|  999|       0|nonexistent|         1.4|        93.918|        -42.7|     4.96|     5228.1| no|  10.0|(11,[10],[1.0])|(12,[10,11],[1.0,...|(12,[10,11],[0.04...|        10|\n| 20|     student| single|      high.school|     no|    yes|  no|telephone|  apr|        thu|      80|       1|  999|       0|nonexistent|        -1.8|        93.075|        -47.1|     1.41|     5099.1| no|  10.0|(11,[10],[1.0])|(12,[10,11],[1.0,...|(12,[10,11],[0.04...|        10|\n| 20|     student| single|      high.school|     no|    yes| yes| cellular|  jul|        tue|     136|       4|  999|       0|nonexistent|         1.4|        93.918|        -42.7|    4.961|     5228.1| no|  10.0|(11,[10],[1.0])|(12,[10,11],[1.0,...|(12,[10,11],[0.04...|        10|\n| 20|     student| single|university.degree|     no|    yes|  no| cellular|  jul|        fri|    1503|      11|  999|       0|nonexistent|         1.4|        93.918|        -42.7|    4.957|     5228.1| no|  10.0|(11,[10],[1.0])|(12,[10,11],[1.0,...|(12,[10,11],[0.04...|        10|\n+---+------------+-------+-----------------+-------+-------+----+---------+-----+-----------+--------+--------+-----+--------+-----------+------------+--------------+-------------+---------+-----------+---+------+---------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1520879233834_-1492798564","id":"20180312-182713_1183626870","dateCreated":"2018-03-12T18:27:13+0000","dateStarted":"2018-03-24T15:44:14+0000","dateFinished":"2018-03-24T15:44:18+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:111"},{"title":"3.2. Outcome rate per cluster","text":"%pyspark\n\noutput_dist = transformed.groupby(\"prediction\").pivot(\"y\").count().cache()\noutput_dist.show()\ntotal = sum(output_dist[c] for c in [\"no\", \"yes\"])\nconversion_rates = output_dist.select(output_dist.columns[0], *[(output_dist[c] / total).alias(c) for c in [\"yes\", \"no\"]]).cache()\nconversion_rates.show()","user":"anonymous","dateUpdated":"2018-03-24T15:44:31+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+----+---+\n|prediction|  no|yes|\n+----------+----+---+\n|        12|2104|343|\n|         1|1049| 91|\n|        13| 247| 20|\n|        16| 739| 85|\n|         6| 396| 39|\n|         3|2416|341|\n|         5|1057| 81|\n|        19|1698|114|\n|        15| 157| 16|\n|         9| 725| 91|\n|        17| 255| 33|\n|         4| 417| 41|\n|         8| 323| 41|\n|         7| 561| 70|\n|        10| 300|134|\n|        11|1543|111|\n|        14| 434| 71|\n|         2| 653|223|\n|         0|3019|400|\n|        18| 140| 16|\n+----------+----+---+\n\n+----------+-------------------+------------------+\n|prediction|                yes|                no|\n+----------+-------------------+------------------+\n|        12|0.14017163874131588|0.8598283612586841|\n|         1|0.07982456140350877|0.9201754385964912|\n|        13| 0.0749063670411985|0.9250936329588015|\n|        16|0.10315533980582524|0.8968446601941747|\n|         6| 0.0896551724137931|0.9103448275862069|\n|         3|0.12368516503445774|0.8763148349655423|\n|         5|0.07117750439367311|0.9288224956063269|\n|        19|0.06291390728476821|0.9370860927152318|\n|        15|0.09248554913294797|0.9075144508670521|\n|         9|0.11151960784313726|0.8884803921568627|\n|        17|0.11458333333333333|0.8854166666666666|\n|         4|0.08951965065502183|0.9104803493449781|\n|         8|0.11263736263736264|0.8873626373626373|\n|         7| 0.1109350237717908|0.8890649762282092|\n|        10| 0.3087557603686636|0.6912442396313364|\n|        11|0.06711003627569528|0.9328899637243047|\n|        14| 0.1405940594059406|0.8594059405940594|\n|         2| 0.2545662100456621|0.7454337899543378|\n|         0|0.11699327288680901| 0.883006727113191|\n|        18|0.10256410256410256|0.8974358974358975|\n+----------+-------------------+------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1520881219815_-114625835","id":"20180312-190019_1904349945","dateCreated":"2018-03-12T19:00:19+0000","dateStarted":"2018-03-24T15:44:31+0000","dateFinished":"2018-03-24T15:44:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:112"},{"title":"3.3. Conversion rate per cluster","text":"%pyspark\nprint('%table\\ncluster number\\tconversion rate\\n'+'\\n'.join(list(map(lambda x: str(x['prediction']) + '\\t' + str(x['yes']), conversion_rates.collect()))))\n","user":"anonymous","dateUpdated":"2018-03-24T15:44:40+0000","config":{"colWidth":6,"enabled":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":274,"optionOpen":false},"helium":{}}},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"cluster number\tconversion rate\n12\t0.14017163874131588\n1\t0.07982456140350877\n13\t0.0749063670411985\n16\t0.10315533980582524\n6\t0.0896551724137931\n3\t0.12368516503445774\n5\t0.07117750439367311\n19\t0.06291390728476821\n15\t0.09248554913294797\n9\t0.11151960784313726\n17\t0.11458333333333333\n4\t0.08951965065502183\n8\t0.11263736263736264\n7\t0.1109350237717908\n10\t0.3087557603686636\n11\t0.06711003627569528\n14\t0.1405940594059406\n2\t0.2545662100456621\n0\t0.11699327288680901\n18\t0.10256410256410256\n"}]},"apps":[],"jobName":"paragraph_1521797000872_502800754","id":"20180323-092320_1234851054","dateCreated":"2018-03-23T09:23:20+0000","dateStarted":"2018-03-24T15:44:40+0000","dateFinished":"2018-03-24T15:44:40+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:113"},{"title":"3.4. Clusters size","text":"%pyspark\nprint('%table\\ncluster number\\tcluster size\\n'+'\\n'.join(list(map(lambda x: str(x['prediction']) + '\\t' + str(x['yes'] + x['no']), output_dist.collect()))))","user":"anonymous","dateUpdated":"2018-03-24T15:44:42+0000","config":{"colWidth":6,"enabled":true,"results":{"0":{"graph":{"mode":"multiBarChart","height":284,"optionOpen":false},"helium":{}}},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"cluster number\tcluster size\n12\t2447\n1\t1140\n13\t267\n16\t824\n6\t435\n3\t2757\n5\t1138\n19\t1812\n15\t173\n9\t816\n17\t288\n4\t458\n8\t364\n7\t631\n10\t434\n11\t1654\n14\t505\n2\t876\n0\t3419\n18\t156\n"}]},"apps":[],"jobName":"paragraph_1521880052044_-1516149022","id":"20180324-082732_1617843410","dateCreated":"2018-03-24T08:27:32+0000","dateStarted":"2018-03-24T15:44:42+0000","dateFinished":"2018-03-24T15:44:42+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:114"},{"title":"3.5. YOUR TURN","text":"%md\n\nTry different values of `k` (number of clusters) and add more features to the clustering pipeline in the step 3.1. Observe how the results in the steps 3.2, 3.3 and 3.4 change with the new values. Can you drive any conclusions regarding the number of clusters needed and the size of the different segments?","user":"anonymous","dateUpdated":"2018-03-24T11:39:25+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Try different values of <code>k</code> (number of clusters) and add more features to the clustering pipeline in the step 3.1. Observe how the results in the steps 3.2, 3.3 and 3.4 change with the new values. Can you drive any conclusions regarding the number of clusters needed and the size of the different segments?</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521883125574_-1249164531","id":"20180324-091845_340120743","dateCreated":"2018-03-24T09:18:45+0000","dateStarted":"2018-03-24T11:39:25+0000","dateFinished":"2018-03-24T11:39:25+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:115"},{"title":"3.6. YOUR TURN","text":"%md\n\nTry a different clustering algorithm in step 3.1. Do the results of the clusters improve?","user":"anonymous","dateUpdated":"2018-03-24T11:37:58+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Try a different clustering algorithm in step 3.1. Do the results of the clusters improve?</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521891370588_-791114199","id":"20180324-113610_1625304988","dateCreated":"2018-03-24T11:36:10+0000","dateStarted":"2018-03-24T11:37:58+0000","dateFinished":"2018-03-24T11:37:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:116"},{"title":"4. Persona creation","text":"%md\n\nLet's get some information about the clusters get created on the previous steps. For example, let's create a kMeans clustering pipeline with 10 clusters and check the average and standard deviation of the age on each cluster:","user":"anonymous","dateUpdated":"2018-03-24T11:44:54+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Let&rsquo;s get some information about the clusters get created on the previous steps. For example, let&rsquo;s create a kMeans clustering pipeline with 10 clusters and check the average and standard deviation of the age on each cluster:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1521891478470_436355741","id":"20180324-113758_1201773249","dateCreated":"2018-03-24T11:37:58+0000","dateStarted":"2018-03-24T11:44:54+0000","dateFinished":"2018-03-24T11:44:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:117"},{"title":"4.1. Clustering pipeline","text":"%pyspark\n\n# Let's do the clustering pipeline again with 10 clusters\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.clustering import KMeans\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, Normalizer\n\n\nstringEncoder = StringIndexer(inputCol=\"job\", outputCol=\"jobInd\")\nencoder = OneHotEncoder(inputCol=\"jobInd\", outputCol=\"jobVec\")\nassembler = VectorAssembler(inputCols=[\"jobVec\", \"age\"],outputCol=\"assembledFeatures\")\nnormalizer = Normalizer(inputCol=\"assembledFeatures\", outputCol=\"features\")\nkmeans = KMeans().setK(5).setSeed(1)\n\npipeline = Pipeline(stages=[stringEncoder, encoder, assembler, normalizer, kmeans])\n\np = pipeline.fit(data)\ntransformed = p.transform(data).cache()","user":"anonymous","dateUpdated":"2018-03-24T15:45:32+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521891894149_-660921286","id":"20180324-114454_851289908","dateCreated":"2018-03-24T11:44:54+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:118","dateFinished":"2018-03-24T15:45:33+0000","dateStarted":"2018-03-24T15:45:32+0000","results":{"code":"SUCCESS","msg":[]}},{"title":"4.2. Age distribution","text":"%pyspark\n\nfrom pyspark.sql import functions as F\n# Let's now compute some stats on the clusters:\n\ntransformed.groupby('prediction').agg(F.avg('age'), F.stddev('age')).sort('prediction').show()","user":"anonymous","dateUpdated":"2018-03-24T15:48:11+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+------------------+------------------+\n|prediction|          avg(age)|  stddev_samp(age)|\n+----------+------------------+------------------+\n|         1|   38.282667179093| 8.954550247147324|\n|         3| 38.42527054694355| 8.567797572551545|\n|         4| 42.03499892311006|11.726884940565467|\n|         2|40.457814661134165| 9.323329044378793|\n|         0| 38.01478083588175| 8.990180244241332|\n+----------+------------------+------------------+\n\n"}]},"apps":[],"jobName":"paragraph_1521892061446_375789821","id":"20180324-114741_1294899047","dateCreated":"2018-03-24T11:47:41+0000","dateStarted":"2018-03-24T15:45:35+0000","dateFinished":"2018-03-24T15:45:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:119"},{"title":"4.3. Job distribution","text":"%pyspark\n\ntransformed.groupby('prediction').pivot('job').count().sort('prediction').show()","user":"anonymous","dateUpdated":"2018-03-24T15:47:55+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+----------+------+-----------+------------+---------+----------+-------+-------------+--------+-------+----------+----------+-------+\n|prediction|admin.|blue-collar|entrepreneur|housemaid|management|retired|self-employed|services|student|technician|unemployed|unknown|\n+----------+------+-----------+------------+---------+----------+-------+-------------+--------+-------+----------+----------+-------+\n|         0|  null|       null|        null|     null|      null|   null|         null|    1962|   null|      null|      null|   null|\n|         1|  5204|       null|        null|     null|      null|   null|         null|    null|   null|      null|      null|   null|\n|         2|  null|       null|        null|     null|      null|   null|          723|    null|   null|      null|      null|   null|\n|         3|  null|       null|        null|     null|      null|   null|         null|    null|   null|      3419|      null|   null|\n|         4|  null|       4606|         725|      537|      1447|    876|         null|    null|    434|      null|       505|    156|\n+----------+------+-----------+------------+---------+----------+-------+-------------+--------+-------+----------+----------+-------+\n\n"}]},"apps":[],"jobName":"paragraph_1521892251777_-1548679850","id":"20180324-115051_2094337","dateCreated":"2018-03-24T11:50:51+0000","dateStarted":"2018-03-24T15:47:56+0000","dateFinished":"2018-03-24T15:47:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:120"},{"text":"%md\nDescribe other features in the data set.\n\n- What can you say about the clusters we obtained?\n- Can you described the customers on each one?","user":"anonymous","dateUpdated":"2018-03-24T15:49:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521894778575_1760425015","id":"20180324-123258_2072435794","dateCreated":"2018-03-24T12:32:58+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:121","dateFinished":"2018-03-24T15:49:33+0000","dateStarted":"2018-03-24T15:49:33+0000","title":"4.4. YOUR TURN","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Describe other features in the data set.</p>\n<ul>\n  <li>What can you say about the clusters we obtained?</li>\n  <li>Can you described the customers on each one?</li>\n</ul>\n</div>"}]}},{"text":"%md\n","user":"anonymous","dateUpdated":"2018-03-24T15:49:33+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521906573211_555678365","id":"20180324-154933_198370562","dateCreated":"2018-03-24T15:49:33+0000","status":"READY","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3148"}],"name":"Task 1","id":"2D9CS78NW","angularObjects":{"2D8MNS82M:shared_process":[],"2D8KQE92P:shared_process":[],"2D8P8URXH:shared_process":[],"2D7GW4WD5:shared_process":[],"2D871HMZA:shared_process":[],"2D6UX3TBT:shared_process":[],"2D886YVQM:shared_process":[],"2D6J3EFVT:shared_process":[],"2D8FHHFU5:shared_process":[],"2D954TTF8:shared_process":[],"2D95FJ579:shared_process":[],"2D84P1U12:shared_process":[],"2D8DHUJVN:shared_process":[],"2D8XGMM1B:shared_process":[],"2D8VBSAXK:shared_process":[],"2D7E5VQ3F:shared_process":[],"2D9KUE39G:shared_process":[],"2D7D9MABB:shared_process":[],"2D5UKYEN4:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}